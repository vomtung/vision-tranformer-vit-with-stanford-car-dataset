{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6YDCIrdnvVgi"
   },
   "source": [
    "- Họ tên: Võ Minh Tùng\n",
    "- MSHV : 2370345"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fb_PwYOouqib"
   },
   "source": [
    "## Vision tranformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7wAxhcxIM_X2"
   },
   "source": [
    "install stanford card dataset keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OmBbm7diurcp",
    "outputId": "bf33f3e0-2807-445c-f980-0df8e446e737"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers==4.46.0 in e:\\software\\python\\lib\\site-packages (4.46.0)\n",
      "Requirement already satisfied: filelock in e:\\software\\python\\lib\\site-packages (from transformers==4.46.0) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in e:\\software\\python\\lib\\site-packages (from transformers==4.46.0) (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.17 in e:\\software\\python\\lib\\site-packages (from transformers==4.46.0) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\vomtu\\appdata\\roaming\\python\\python312\\site-packages (from transformers==4.46.0) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in e:\\software\\python\\lib\\site-packages (from transformers==4.46.0) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in e:\\software\\python\\lib\\site-packages (from transformers==4.46.0) (2024.9.11)\n",
      "Requirement already satisfied: requests in e:\\software\\python\\lib\\site-packages (from transformers==4.46.0) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in e:\\software\\python\\lib\\site-packages (from transformers==4.46.0) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in e:\\software\\python\\lib\\site-packages (from transformers==4.46.0) (0.20.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in e:\\software\\python\\lib\\site-packages (from transformers==4.46.0) (4.66.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in e:\\software\\python\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.46.0) (2024.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in e:\\software\\python\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.46.0) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\vomtu\\appdata\\roaming\\python\\python312\\site-packages (from tqdm>=4.27->transformers==4.46.0) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in e:\\software\\python\\lib\\site-packages (from requests->transformers==4.46.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in e:\\software\\python\\lib\\site-packages (from requests->transformers==4.46.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in e:\\software\\python\\lib\\site-packages (from requests->transformers==4.46.0) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in e:\\software\\python\\lib\\site-packages (from requests->transformers==4.46.0) (2024.8.30)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers==4.46.0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "ERROR: Invalid requirement: \"'transformers[flax]'\": Expected package name at the start of dependency specifier\n",
      "    'transformers[flax]'\n",
      "    ^\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade tensorflow\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement tensorflow==2.11 (from versions: 2.16.0rc0, 2.16.1, 2.16.2, 2.17.0rc0, 2.17.0rc1, 2.17.0, 2.17.1, 2.18.0rc0, 2.18.0rc1, 2.18.0rc2, 2.18.0)\n",
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "ERROR: No matching distribution found for tensorflow==2.11\n"
     ]
    }
   ],
   "source": [
    "%pip install tensorflow==2.11 keras==2.11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.12.7 (tags/v3.12.7:0b05ead, Oct  1 2024, 03:06:41) [MSC v.1941 64 bit (AMD64)]\n",
      "TensorFlow version: 2.18.0\n",
      "Keras version: 3.5.0\n",
      "Transformers library version: 4.46.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import transformers\n",
    "import sys\n",
    "\n",
    "print(\"Python version:\", sys.version)\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"Keras version:\", keras.__version__)\n",
    "print(\"Transformers library version:\", transformers.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YQrzOcLuN6RV"
   },
   "source": [
    "Chuẩn bị Bộ Dữ Liệu Stanford Cars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "qo7vSN7KN-9P",
    "outputId": "c04ca51f-ad24-4db4-ccca-c9ef4ea07c78"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\software\\python\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From e:\\software\\python\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "Found 16185 files belonging to 2 classes.\n",
      "Using 12948 files for training.\n",
      "Found 16185 files belonging to 2 classes.\n",
      "Using 3237 files for validation.\n",
      "Image batch shape: (32, 224, 224, 3)\n",
      "Label batch shape: (32,)\n",
      "Image data type: <dtype: 'float32'>\n",
      "Label data type: <dtype: 'int32'>\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import TFViTForImageClassification\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Define dataset directory and parameters\n",
    "data_dir = \"./stanford_cars\"\n",
    "batch_size = 32\n",
    "img_size = (224, 224)\n",
    "\n",
    "# Load the training and validation datasets\n",
    "train_ds = image_dataset_from_directory(\n",
    "    data_dir,\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\",\n",
    "    seed=123,\n",
    "    image_size=img_size,\n",
    "    batch_size=batch_size,\n",
    ")\n",
    "\n",
    "val_ds = image_dataset_from_directory(\n",
    "    data_dir,\n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\",\n",
    "    seed=123,\n",
    "    image_size=img_size,\n",
    "    batch_size=batch_size,\n",
    ")\n",
    "\n",
    "# Retrieve class names and set the number of classes\n",
    "class_names = train_ds.class_names\n",
    "num_classes = len(class_names)\n",
    "\n",
    "# Define a preprocessing function\n",
    "def preprocess(image, label):\n",
    "    image = tf.image.resize(image, img_size)  # Resize to (224, 224)\n",
    "    image = tf.cast(image, tf.float32) / 255.0  # Normalize to [0, 1]\n",
    "    return image, label\n",
    "\n",
    "# Apply preprocessing\n",
    "train_ds = train_ds.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE).cache().prefetch(tf.data.AUTOTUNE)\n",
    "val_ds = val_ds.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE).cache().prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Debugging: Print data type and shape information\n",
    "for images, labels in train_ds.take(1):\n",
    "    print(\"Image batch shape:\", images.shape)\n",
    "    print(\"Label batch shape:\", labels.shape)\n",
    "    print(\"Image data type:\", images.dtype)\n",
    "    print(\"Label data type:\", labels.dtype)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFViTForImageClassification.\n",
      "\n",
      "All the weights of TFViTForImageClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFViTForImageClassification for predictions without further training.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Could not interpret optimizer identifier: <keras.src.optimizers.adam.Adam object at 0x000001C2F3BB3950>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 7\u001b[0m\n\u001b[0;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m TFViTForImageClassification\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgoogle/vit-base-patch16-224\u001b[39m\u001b[38;5;124m\"\u001b[39m, attn_implementation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msdpa\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      4\u001b[0m )\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Compile the model\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mAdam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5e-5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlosses\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSparseCategoricalCrossentropy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfrom_logits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maccuracy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\software\\python\\Lib\\site-packages\\transformers\\modeling_tf_utils.py:1563\u001b[0m, in \u001b[0;36mTFPreTrainedModel.compile\u001b[1;34m(self, optimizer, loss, metrics, loss_weights, weighted_metrics, run_eagerly, steps_per_execution, **kwargs)\u001b[0m\n\u001b[0;32m   1561\u001b[0m \u001b[38;5;66;03m# This argument got renamed, we need to support both versions\u001b[39;00m\n\u001b[0;32m   1562\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msteps_per_execution\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m parent_args:\n\u001b[1;32m-> 1563\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1565\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1566\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1567\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloss_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1568\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweighted_metrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweighted_metrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1569\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_eagerly\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_eagerly\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1570\u001b[0m \u001b[43m        \u001b[49m\u001b[43msteps_per_execution\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps_per_execution\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1571\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1572\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1573\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1574\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[0;32m   1575\u001b[0m         optimizer\u001b[38;5;241m=\u001b[39moptimizer,\n\u001b[0;32m   1576\u001b[0m         loss\u001b[38;5;241m=\u001b[39mloss,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1582\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   1583\u001b[0m     )\n",
      "File \u001b[1;32me:\\software\\python\\Lib\\site-packages\\tf_keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32me:\\software\\python\\Lib\\site-packages\\tf_keras\\src\\optimizers\\__init__.py:335\u001b[0m, in \u001b[0;36mget\u001b[1;34m(identifier, **kwargs)\u001b[0m\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m get(\n\u001b[0;32m    331\u001b[0m         config,\n\u001b[0;32m    332\u001b[0m         use_legacy_optimizer\u001b[38;5;241m=\u001b[39muse_legacy_optimizer,\n\u001b[0;32m    333\u001b[0m     )\n\u001b[0;32m    334\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 335\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    336\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not interpret optimizer identifier: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midentifier\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    337\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Could not interpret optimizer identifier: <keras.src.optimizers.adam.Adam object at 0x000001C2F3BB3950>"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize a ViT model from the transformers library\n",
    "model = TFViTForImageClassification.from_pretrained(\n",
    "    \"google/vit-base-patch16-224\", attn_implementation=\"sdpa\"\n",
    ")\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=5e-5),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_vi_t_for_image_classification\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " vit (TFViTMainLayer)        multiple                  85798656  \n",
      "                                                                 \n",
      " classifier (Dense)          multiple                  769000    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 86567656 (330.23 MB)\n",
      "Trainable params: 86567656 (330.23 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 224, 224, 3) (32,)\n",
      "(32, 224, 224, 3) (32,)\n",
      "(32, 224, 224, 3) (32,)\n",
      "(32, 224, 224, 3) (32,)\n",
      "(32, 224, 224, 3) (32,)\n",
      "(32, 224, 224, 3) (32,)\n",
      "(32, 224, 224, 3) (32,)\n",
      "(32, 224, 224, 3) (32,)\n",
      "(32, 224, 224, 3) (32,)\n",
      "(32, 224, 224, 3) (32,)\n"
     ]
    }
   ],
   "source": [
    "for images, labels in train_ds.take(10):\n",
    "    print(images.shape, labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 224, 224, 3) (32,)\n",
      "(32, 224, 224, 3) (32,)\n",
      "(32, 224, 224, 3) (32,)\n",
      "(32, 224, 224, 3) (32,)\n",
      "(32, 224, 224, 3) (32,)\n",
      "(32, 224, 224, 3) (32,)\n",
      "(32, 224, 224, 3) (32,)\n",
      "(32, 224, 224, 3) (32,)\n",
      "(32, 224, 224, 3) (32,)\n",
      "(32, 224, 224, 3) (32,)\n"
     ]
    }
   ],
   "source": [
    "for images, labels in val_ds.take(10):\n",
    "    print(images.shape, labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"e:\\software\\python\\Lib\\site-packages\\tf_keras\\src\\engine\\training.py\", line 1398, in train_function  *\n        return step_function(self, iterator)\n    File \"e:\\software\\python\\Lib\\site-packages\\tf_keras\\src\\engine\\training.py\", line 1381, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"e:\\software\\python\\Lib\\site-packages\\tf_keras\\src\\engine\\training.py\", line 1370, in run_step  **\n        outputs = model.train_step(data)\n    File \"e:\\software\\python\\Lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 1672, in train_step\n        y_pred = self(x, training=True)\n    File \"e:\\software\\python\\Lib\\site-packages\\tf_keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\vomtu\\AppData\\Local\\Temp\\__autograph_generated_filex0zjeq7o.py\", line 40, in tf__run_call_with_unpacked_inputs\n        raise\n    File \"C:\\Users\\vomtu\\AppData\\Local\\Temp\\__autograph_generated_fileqi34xc38.py\", line 17, in tf__call\n        outputs = ag__.converted_call(ag__.ld(self).vit, (), dict(pixel_values=ag__.ld(pixel_values), head_mask=ag__.ld(head_mask), output_attentions=ag__.ld(output_attentions), output_hidden_states=ag__.ld(output_hidden_states), interpolate_pos_encoding=ag__.ld(interpolate_pos_encoding), return_dict=ag__.ld(return_dict), training=ag__.ld(training)), fscope)\n    File \"C:\\Users\\vomtu\\AppData\\Local\\Temp\\__autograph_generated_filex0zjeq7o.py\", line 40, in tf__run_call_with_unpacked_inputs\n        raise\n    File \"C:\\Users\\vomtu\\AppData\\Local\\Temp\\__autograph_generated_filexlnfz_wa.py\", line 24, in tf__call\n        embedding_output = ag__.converted_call(ag__.ld(self).embeddings, (), dict(pixel_values=ag__.ld(pixel_values), interpolate_pos_encoding=ag__.ld(interpolate_pos_encoding), training=ag__.ld(training)), fscope)\n    File \"C:\\Users\\vomtu\\AppData\\Local\\Temp\\__autograph_generated_filebum8ks0a.py\", line 12, in tf__call\n        embeddings = ag__.converted_call(ag__.ld(self).patch_embeddings, (ag__.ld(pixel_values),), dict(interpolate_pos_encoding=ag__.ld(interpolate_pos_encoding), training=ag__.ld(training)), fscope)\n    File \"C:\\Users\\vomtu\\AppData\\Local\\Temp\\__autograph_generated_filey57e1jiy.py\", line 63, in tf__call\n        projection = ag__.converted_call(ag__.ld(self).projection, (ag__.ld(pixel_values),), None, fscope)\n\n    ValueError: Exception encountered when calling layer 'tf_vi_t_for_image_classification' (type TFViTForImageClassification).\n    \n    in user code:\n    \n        File \"e:\\software\\python\\Lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 862, in run_call_with_unpacked_inputs  *\n            return func(self, **unpacked_inputs)\n        File \"e:\\software\\python\\Lib\\site-packages\\transformers\\models\\vit\\modeling_tf_vit.py\", line 871, in call  *\n            outputs = self.vit(\n        File \"e:\\software\\python\\Lib\\site-packages\\tf_keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler  **\n            raise e.with_traceback(filtered_tb) from None\n        File \"C:\\Users\\vomtu\\AppData\\Local\\Temp\\__autograph_generated_filex0zjeq7o.py\", line 40, in tf__run_call_with_unpacked_inputs\n            raise\n        File \"C:\\Users\\vomtu\\AppData\\Local\\Temp\\__autograph_generated_filexlnfz_wa.py\", line 24, in tf__call\n            embedding_output = ag__.converted_call(ag__.ld(self).embeddings, (), dict(pixel_values=ag__.ld(pixel_values), interpolate_pos_encoding=ag__.ld(interpolate_pos_encoding), training=ag__.ld(training)), fscope)\n        File \"C:\\Users\\vomtu\\AppData\\Local\\Temp\\__autograph_generated_filebum8ks0a.py\", line 12, in tf__call\n            embeddings = ag__.converted_call(ag__.ld(self).patch_embeddings, (ag__.ld(pixel_values),), dict(interpolate_pos_encoding=ag__.ld(interpolate_pos_encoding), training=ag__.ld(training)), fscope)\n        File \"C:\\Users\\vomtu\\AppData\\Local\\Temp\\__autograph_generated_filey57e1jiy.py\", line 63, in tf__call\n            projection = ag__.converted_call(ag__.ld(self).projection, (ag__.ld(pixel_values),), None, fscope)\n    \n        ValueError: Exception encountered when calling layer 'vit' (type TFViTMainLayer).\n        \n        in user code:\n        \n            File \"e:\\software\\python\\Lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 862, in run_call_with_unpacked_inputs  *\n                return func(self, **unpacked_inputs)\n            File \"e:\\software\\python\\Lib\\site-packages\\transformers\\models\\vit\\modeling_tf_vit.py\", line 595, in call  *\n                embedding_output = self.embeddings(\n            File \"e:\\software\\python\\Lib\\site-packages\\tf_keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler  **\n                raise e.with_traceback(filtered_tb) from None\n            File \"C:\\Users\\vomtu\\AppData\\Local\\Temp\\__autograph_generated_filebum8ks0a.py\", line 12, in tf__call\n                embeddings = ag__.converted_call(ag__.ld(self).patch_embeddings, (ag__.ld(pixel_values),), dict(interpolate_pos_encoding=ag__.ld(interpolate_pos_encoding), training=ag__.ld(training)), fscope)\n            File \"C:\\Users\\vomtu\\AppData\\Local\\Temp\\__autograph_generated_filey57e1jiy.py\", line 63, in tf__call\n                projection = ag__.converted_call(ag__.ld(self).projection, (ag__.ld(pixel_values),), None, fscope)\n        \n            ValueError: Exception encountered when calling layer 'embeddings' (type TFViTEmbeddings).\n            \n            in user code:\n            \n                File \"e:\\software\\python\\Lib\\site-packages\\transformers\\models\\vit\\modeling_tf_vit.py\", line 129, in call  *\n                    embeddings = self.patch_embeddings(\n                File \"e:\\software\\python\\Lib\\site-packages\\tf_keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler  **\n                    raise e.with_traceback(filtered_tb) from None\n                File \"C:\\Users\\vomtu\\AppData\\Local\\Temp\\__autograph_generated_filey57e1jiy.py\", line 63, in tf__call\n                    projection = ag__.converted_call(ag__.ld(self).projection, (ag__.ld(pixel_values),), None, fscope)\n            \n                ValueError: Exception encountered when calling layer 'patch_embeddings' (type TFViTPatchEmbeddings).\n                \n                in user code:\n                \n                    File \"e:\\software\\python\\Lib\\site-packages\\transformers\\models\\vit\\modeling_tf_vit.py\", line 204, in call  *\n                        projection = self.projection(pixel_values)\n                    File \"e:\\software\\python\\Lib\\site-packages\\tf_keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler  **\n                        raise e.with_traceback(filtered_tb) from None\n                    File \"e:\\software\\python\\Lib\\site-packages\\tf_keras\\src\\engine\\input_spec.py\", line 280, in assert_input_compatibility\n                        raise ValueError(\n                \n                    ValueError: Input 0 of layer \"projection\" is incompatible with the layer: expected axis -1 of input shape to have value 3, but received input with shape (None, 224, 3, 224)\n                \n                \n                Call arguments received by layer 'patch_embeddings' (type TFViTPatchEmbeddings):\n                  • pixel_values=tf.Tensor(shape=(None, 224, 224, 3), dtype=float32)\n                  • interpolate_pos_encoding=None\n                  • training=True\n            \n            \n            Call arguments received by layer 'embeddings' (type TFViTEmbeddings):\n              • pixel_values=tf.Tensor(shape=(None, 224, 224, 3), dtype=float32)\n              • interpolate_pos_encoding=None\n              • training=True\n        \n        \n        Call arguments received by layer 'vit' (type TFViTMainLayer):\n          • pixel_values=tf.Tensor(shape=(None, 224, 224, 3), dtype=float32)\n          • head_mask=None\n          • output_attentions=False\n          • output_hidden_states=False\n          • interpolate_pos_encoding=None\n          • return_dict=True\n          • training=True\n    \n    \n    Call arguments received by layer 'tf_vi_t_for_image_classification' (type TFViTForImageClassification):\n      • pixel_values=tf.Tensor(shape=(None, 224, 224, 3), dtype=float32)\n      • head_mask=None\n      • output_attentions=None\n      • output_hidden_states=None\n      • interpolate_pos_encoding=None\n      • return_dict=None\n      • labels=None\n      • training=True\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_ds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\n\u001b[0;32m      6\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\software\\python\\Lib\\site-packages\\transformers\\modeling_tf_utils.py:1229\u001b[0m, in \u001b[0;36mTFPreTrainedModel.fit\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1226\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(keras\u001b[38;5;241m.\u001b[39mModel\u001b[38;5;241m.\u001b[39mfit)\n\u001b[0;32m   1227\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m   1228\u001b[0m     args, kwargs \u001b[38;5;241m=\u001b[39m convert_batch_encoding(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m-> 1229\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\software\\python\\Lib\\site-packages\\tf_keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filemfhykf2v.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32me:\\software\\python\\Lib\\site-packages\\transformers\\modeling_tf_utils.py:1672\u001b[0m, in \u001b[0;36mTFPreTrainedModel.train_step\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m   1670\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m(x, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, return_loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   1671\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1672\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m   1673\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_using_dummy_loss:\n\u001b[0;32m   1674\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompiled_loss(y_pred\u001b[38;5;241m.\u001b[39mloss, y_pred\u001b[38;5;241m.\u001b[39mloss, sample_weight, regularization_losses\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlosses)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filex0zjeq7o.py:37\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__run_call_with_unpacked_inputs\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     36\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(func), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m),), \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(unpacked_inputs)), fscope)\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     39\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_fileqi34xc38.py:17\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call\u001b[1;34m(self, pixel_values, head_mask, output_attentions, output_hidden_states, interpolate_pos_encoding, return_dict, labels, training)\u001b[0m\n\u001b[0;32m     15\u001b[0m do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     16\u001b[0m retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefinedReturnValue()\n\u001b[1;32m---> 17\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolate_pos_encoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43minterpolate_pos_encoding\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(outputs)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     19\u001b[0m logits \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mclassifier, (), \u001b[38;5;28mdict\u001b[39m(inputs\u001b[38;5;241m=\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(sequence_output)[:, \u001b[38;5;241m0\u001b[39m, :]), fscope)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filex0zjeq7o.py:37\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__run_call_with_unpacked_inputs\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     36\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(func), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m),), \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(unpacked_inputs)), fscope)\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     39\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filexlnfz_wa.py:24\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call\u001b[1;34m(self, pixel_values, head_mask, output_attentions, output_hidden_states, interpolate_pos_encoding, return_dict, training)\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m     23\u001b[0m ag__\u001b[38;5;241m.\u001b[39mif_stmt(ag__\u001b[38;5;241m.\u001b[39mld(pixel_values) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, if_body, else_body, get_state, set_state, (), \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m---> 24\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolate_pos_encoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43minterpolate_pos_encoding\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_state_1\u001b[39m():\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (head_mask,)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filebum8ks0a.py:12\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call\u001b[1;34m(self, pixel_values, interpolate_pos_encoding, training)\u001b[0m\n\u001b[0;32m     10\u001b[0m retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefinedReturnValue()\n\u001b[0;32m     11\u001b[0m batch_size, num_channels, height, width \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(shape_list), (ag__\u001b[38;5;241m.\u001b[39mld(pixel_values),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m---> 12\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpatch_embeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minterpolate_pos_encoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43minterpolate_pos_encoding\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m cls_tokens \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mrepeat, (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mcls_token,), \u001b[38;5;28mdict\u001b[39m(repeats\u001b[38;5;241m=\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(batch_size), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m), fscope)\n\u001b[0;32m     14\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mconcat, ((ag__\u001b[38;5;241m.\u001b[39mld(cls_tokens), ag__\u001b[38;5;241m.\u001b[39mld(embeddings)),), \u001b[38;5;28mdict\u001b[39m(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m), fscope)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filey57e1jiy.py:63\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call\u001b[1;34m(self, pixel_values, interpolate_pos_encoding, training)\u001b[0m\n\u001b[0;32m     61\u001b[0m ag__\u001b[38;5;241m.\u001b[39mif_stmt(ag__\u001b[38;5;241m.\u001b[39mnot_(ag__\u001b[38;5;241m.\u001b[39mld(interpolate_pos_encoding)), if_body_3, else_body_3, get_state_3, set_state_3, (), \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     62\u001b[0m pixel_values \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mtranspose, (ag__\u001b[38;5;241m.\u001b[39mld(pixel_values),), \u001b[38;5;28mdict\u001b[39m(perm\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m)), fscope)\n\u001b[1;32m---> 63\u001b[0m projection \u001b[38;5;241m=\u001b[39m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprojection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     64\u001b[0m num_patches \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(width) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mpatch_size[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m*\u001b[39m (ag__\u001b[38;5;241m.\u001b[39mld(height) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mpatch_size[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m     65\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mreshape, (), \u001b[38;5;28mdict\u001b[39m(tensor\u001b[38;5;241m=\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(projection), shape\u001b[38;5;241m=\u001b[39m(ag__\u001b[38;5;241m.\u001b[39mld(batch_size), ag__\u001b[38;5;241m.\u001b[39mld(num_patches), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)), fscope)\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"e:\\software\\python\\Lib\\site-packages\\tf_keras\\src\\engine\\training.py\", line 1398, in train_function  *\n        return step_function(self, iterator)\n    File \"e:\\software\\python\\Lib\\site-packages\\tf_keras\\src\\engine\\training.py\", line 1381, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"e:\\software\\python\\Lib\\site-packages\\tf_keras\\src\\engine\\training.py\", line 1370, in run_step  **\n        outputs = model.train_step(data)\n    File \"e:\\software\\python\\Lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 1672, in train_step\n        y_pred = self(x, training=True)\n    File \"e:\\software\\python\\Lib\\site-packages\\tf_keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\vomtu\\AppData\\Local\\Temp\\__autograph_generated_filex0zjeq7o.py\", line 40, in tf__run_call_with_unpacked_inputs\n        raise\n    File \"C:\\Users\\vomtu\\AppData\\Local\\Temp\\__autograph_generated_fileqi34xc38.py\", line 17, in tf__call\n        outputs = ag__.converted_call(ag__.ld(self).vit, (), dict(pixel_values=ag__.ld(pixel_values), head_mask=ag__.ld(head_mask), output_attentions=ag__.ld(output_attentions), output_hidden_states=ag__.ld(output_hidden_states), interpolate_pos_encoding=ag__.ld(interpolate_pos_encoding), return_dict=ag__.ld(return_dict), training=ag__.ld(training)), fscope)\n    File \"C:\\Users\\vomtu\\AppData\\Local\\Temp\\__autograph_generated_filex0zjeq7o.py\", line 40, in tf__run_call_with_unpacked_inputs\n        raise\n    File \"C:\\Users\\vomtu\\AppData\\Local\\Temp\\__autograph_generated_filexlnfz_wa.py\", line 24, in tf__call\n        embedding_output = ag__.converted_call(ag__.ld(self).embeddings, (), dict(pixel_values=ag__.ld(pixel_values), interpolate_pos_encoding=ag__.ld(interpolate_pos_encoding), training=ag__.ld(training)), fscope)\n    File \"C:\\Users\\vomtu\\AppData\\Local\\Temp\\__autograph_generated_filebum8ks0a.py\", line 12, in tf__call\n        embeddings = ag__.converted_call(ag__.ld(self).patch_embeddings, (ag__.ld(pixel_values),), dict(interpolate_pos_encoding=ag__.ld(interpolate_pos_encoding), training=ag__.ld(training)), fscope)\n    File \"C:\\Users\\vomtu\\AppData\\Local\\Temp\\__autograph_generated_filey57e1jiy.py\", line 63, in tf__call\n        projection = ag__.converted_call(ag__.ld(self).projection, (ag__.ld(pixel_values),), None, fscope)\n\n    ValueError: Exception encountered when calling layer 'tf_vi_t_for_image_classification' (type TFViTForImageClassification).\n    \n    in user code:\n    \n        File \"e:\\software\\python\\Lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 862, in run_call_with_unpacked_inputs  *\n            return func(self, **unpacked_inputs)\n        File \"e:\\software\\python\\Lib\\site-packages\\transformers\\models\\vit\\modeling_tf_vit.py\", line 871, in call  *\n            outputs = self.vit(\n        File \"e:\\software\\python\\Lib\\site-packages\\tf_keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler  **\n            raise e.with_traceback(filtered_tb) from None\n        File \"C:\\Users\\vomtu\\AppData\\Local\\Temp\\__autograph_generated_filex0zjeq7o.py\", line 40, in tf__run_call_with_unpacked_inputs\n            raise\n        File \"C:\\Users\\vomtu\\AppData\\Local\\Temp\\__autograph_generated_filexlnfz_wa.py\", line 24, in tf__call\n            embedding_output = ag__.converted_call(ag__.ld(self).embeddings, (), dict(pixel_values=ag__.ld(pixel_values), interpolate_pos_encoding=ag__.ld(interpolate_pos_encoding), training=ag__.ld(training)), fscope)\n        File \"C:\\Users\\vomtu\\AppData\\Local\\Temp\\__autograph_generated_filebum8ks0a.py\", line 12, in tf__call\n            embeddings = ag__.converted_call(ag__.ld(self).patch_embeddings, (ag__.ld(pixel_values),), dict(interpolate_pos_encoding=ag__.ld(interpolate_pos_encoding), training=ag__.ld(training)), fscope)\n        File \"C:\\Users\\vomtu\\AppData\\Local\\Temp\\__autograph_generated_filey57e1jiy.py\", line 63, in tf__call\n            projection = ag__.converted_call(ag__.ld(self).projection, (ag__.ld(pixel_values),), None, fscope)\n    \n        ValueError: Exception encountered when calling layer 'vit' (type TFViTMainLayer).\n        \n        in user code:\n        \n            File \"e:\\software\\python\\Lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 862, in run_call_with_unpacked_inputs  *\n                return func(self, **unpacked_inputs)\n            File \"e:\\software\\python\\Lib\\site-packages\\transformers\\models\\vit\\modeling_tf_vit.py\", line 595, in call  *\n                embedding_output = self.embeddings(\n            File \"e:\\software\\python\\Lib\\site-packages\\tf_keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler  **\n                raise e.with_traceback(filtered_tb) from None\n            File \"C:\\Users\\vomtu\\AppData\\Local\\Temp\\__autograph_generated_filebum8ks0a.py\", line 12, in tf__call\n                embeddings = ag__.converted_call(ag__.ld(self).patch_embeddings, (ag__.ld(pixel_values),), dict(interpolate_pos_encoding=ag__.ld(interpolate_pos_encoding), training=ag__.ld(training)), fscope)\n            File \"C:\\Users\\vomtu\\AppData\\Local\\Temp\\__autograph_generated_filey57e1jiy.py\", line 63, in tf__call\n                projection = ag__.converted_call(ag__.ld(self).projection, (ag__.ld(pixel_values),), None, fscope)\n        \n            ValueError: Exception encountered when calling layer 'embeddings' (type TFViTEmbeddings).\n            \n            in user code:\n            \n                File \"e:\\software\\python\\Lib\\site-packages\\transformers\\models\\vit\\modeling_tf_vit.py\", line 129, in call  *\n                    embeddings = self.patch_embeddings(\n                File \"e:\\software\\python\\Lib\\site-packages\\tf_keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler  **\n                    raise e.with_traceback(filtered_tb) from None\n                File \"C:\\Users\\vomtu\\AppData\\Local\\Temp\\__autograph_generated_filey57e1jiy.py\", line 63, in tf__call\n                    projection = ag__.converted_call(ag__.ld(self).projection, (ag__.ld(pixel_values),), None, fscope)\n            \n                ValueError: Exception encountered when calling layer 'patch_embeddings' (type TFViTPatchEmbeddings).\n                \n                in user code:\n                \n                    File \"e:\\software\\python\\Lib\\site-packages\\transformers\\models\\vit\\modeling_tf_vit.py\", line 204, in call  *\n                        projection = self.projection(pixel_values)\n                    File \"e:\\software\\python\\Lib\\site-packages\\tf_keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler  **\n                        raise e.with_traceback(filtered_tb) from None\n                    File \"e:\\software\\python\\Lib\\site-packages\\tf_keras\\src\\engine\\input_spec.py\", line 280, in assert_input_compatibility\n                        raise ValueError(\n                \n                    ValueError: Input 0 of layer \"projection\" is incompatible with the layer: expected axis -1 of input shape to have value 3, but received input with shape (None, 224, 3, 224)\n                \n                \n                Call arguments received by layer 'patch_embeddings' (type TFViTPatchEmbeddings):\n                  • pixel_values=tf.Tensor(shape=(None, 224, 224, 3), dtype=float32)\n                  • interpolate_pos_encoding=None\n                  • training=True\n            \n            \n            Call arguments received by layer 'embeddings' (type TFViTEmbeddings):\n              • pixel_values=tf.Tensor(shape=(None, 224, 224, 3), dtype=float32)\n              • interpolate_pos_encoding=None\n              • training=True\n        \n        \n        Call arguments received by layer 'vit' (type TFViTMainLayer):\n          • pixel_values=tf.Tensor(shape=(None, 224, 224, 3), dtype=float32)\n          • head_mask=None\n          • output_attentions=False\n          • output_hidden_states=False\n          • interpolate_pos_encoding=None\n          • return_dict=True\n          • training=True\n    \n    \n    Call arguments received by layer 'tf_vi_t_for_image_classification' (type TFViTForImageClassification):\n      • pixel_values=tf.Tensor(shape=(None, 224, 224, 3), dtype=float32)\n      • head_mask=None\n      • output_attentions=None\n      • output_hidden_states=None\n      • interpolate_pos_encoding=None\n      • return_dict=None\n      • labels=None\n      • training=True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=5\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
